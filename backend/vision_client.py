"""
Vision API wrapper for multimodal model integration.
Handles streaming communication with the multimodal API.
"""
import os
import json
import requests
from typing import Dict, List, Any, Iterator
from dotenv import load_dotenv

# Load environment variables
load_dotenv()


class VisionLLMClient:
    """
    Minimal wrapper for the hosted vision model.
    Responsibility: Stream tokens from the Vision API, nothing else.
    """
    
    def __init__(self):
        """Initialize with API credentials from environment."""
        self.api_key = os.getenv("VISION_API_KEY")
        self.api_base = os.getenv(
            "VISION_API_BASE", 
            "https://platform.qubrid.com/api/v1/qubridai/multimodal/chat"
        )
        self.model_name = "Qwen/Qwen3-VL-30B-A3B-Instruct"
        
        if not self.api_key:
            raise ValueError("VISION_API_KEY must be set in .env file")
    
    def stream(
        self, 
        messages: List[Dict[str, Any]], 
        temperature: float = 0.7,
        max_tokens: int = 1024,
        top_p: float = 0.9,
        top_k: int = 40,
        presence_penalty: float = 0.0,
        **kwargs
    ) -> Iterator[str]:
        """
        Stream tokens from the Vision API.
        
        Args:
            messages: List of message dicts in OpenAI format
            temperature: Sampling temperature (0.0-2.0)
            max_tokens: Maximum tokens to generate
            top_p: Nucleus sampling threshold
            top_k: Top-k sampling limit
            presence_penalty: Penalty for token presence
            
        Yields:
            Content chunks as they arrive from the API
            
        Raises:
            requests.HTTPError: If API request fails
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        
        payload = {
            "model": self.model_name,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": top_p,
            "stream": True,
        }
        
        response = requests.post(
            self.api_base, 
            headers=headers, 
            json=payload, 
            stream=True,
            timeout=60
        )
        if response.status_code != 200:
            error_msg = f"API Error {response.status_code}: {response.text}"
            print(error_msg)
            raise ValueError(error_msg)
        
        # Parse Server-Sent Events (SSE)
        for line in response.iter_lines():
            if not line:
                continue
                
            decoded_line = line.decode("utf-8")
            
            # SSE format: "data: {json}"
            if not decoded_line.startswith("data: "):
                continue
            
            json_str = decoded_line[6:]  # Remove "data: " prefix
            
            # Check for stream end signal
            if json_str.strip() == "[DONE]":
                break
            
            # Parse and extract content
            try:
                chunk = json.loads(json_str)
                content = chunk["choices"][0]["delta"].get("content", "")
                if content:
                    yield content
            except (json.JSONDecodeError, KeyError, IndexError):
                # Skip malformed chunks
                continue
